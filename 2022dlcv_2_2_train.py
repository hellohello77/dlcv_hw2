# -*- coding: utf-8 -*-
"""「2022dlcv_2_2.ipynb」的副本

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uxcODG_DDXJyyIgXwWZc0QC-QUCWnUy2

# Preparation
"""

# torch.cuda.empty_cache()
'''
source: https://github.com/TeaPearce/Conditional_Diffusion_MNIST/blob/main/script.py?fbclid=IwAR3pBmDkgAgzrGesBSOaXYqIaj-9tz2Q1Nb7NapVX8p2OJD2svFd8dbWf6A
'''

"""## Check GPU Type"""

# !nvidia-smi

# from google.colab import drive
# drive.mount('/content/drive')

"""## Download Data"""

# !gdown --id 1YxkObGDlqZM0-9Zq-QMjk7q1vND4UJl3 --output "./hw2_data.zip"
# !gdown --id 1WWeGODCVxaWzMhDlP4Pq-Z8ad7moy_Oz
# !unzip -q "./hw2_data.zip" -d "./"
# !rm hw2_data.zip

"""## Packages"""

# Commented out IPython magic to ensure Python compatibility.
import torch
import PIL
from PIL import Image
import matplotlib.pyplot as plt
import os
from torch.utils.data import DataLoader
import torchvision
from torchvision import models
import torch.nn as nn
import numpy as np
import csv
from typing import Dict, Tuple
from tqdm.notebook import tqdm
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torchvision import models, transforms
from torchvision.datasets import MNIST
from torchvision.utils import save_image, make_grid
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation, PillowWriter
import numpy as np
# print(torch.backends.mps.is_available())
# %matplotlib inline

path_to_datafile = '/content/hw2_data/digits'

# img=Image.open(os.path.join(path_to_datafile, 'p1_data/train_50/0_0.png'))
# plt.imshow(img)
# plt.show()

"""# Dataset

## Dataset
"""

class hw2_2_dataset:
    def __init__(self, filepath, transform):
        self.transform = transform
        self.filepath = filepath
        self.file_list = []
        self.labels = []
        with open(os.path.join(filepath, 'train.csv'), newline='') as csvfile:
          reader = csv.reader(csvfile, delimiter=',')
          for idx, row in enumerate(reader):
            if idx:
              self.file_list.append(row[0])
              self.labels.append(int(row[1]))
        with open(os.path.join(filepath, 'val.csv'), newline='') as csvfile:
          reader = csv.reader(csvfile, delimiter=',')
          for idx, row in enumerate(reader):
            if idx:
              self.file_list.append(row[0])
              self.labels.append(int(row[1]))
    
    def __len__(self):
        return len(self.file_list)
    
    def __getitem__(self, idx):
        img_path = os.path.join(self.filepath, 'data', self.file_list[idx])
        img = Image.open(img_path)
        transformed_img = self.transform(img)
        img.close()
        return transformed_img, self.labels[idx]

"""## Data Loader"""

import math
img_transform = transforms.Compose([
    # T.Resize(224),
    transforms.ToTensor(),
    # transforms.Normalize(mean=[0.5, 0.5, 0.5],
    #            std=[0.5, 0.5, 0.5])
])
dataset = hw2_2_dataset(os.path.join(path_to_datafile, 'mnistm'), img_transform)
BATCH_SIZE = 32
dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)

"""# Model

## UNet
"""

class ResidualConvBlock(nn.Module):
    def __init__(
        self, in_channels: int, out_channels: int, is_res: bool = False
    ) -> None:
        super().__init__()
        '''
        standard ResNet style convolutional block
        '''
        self.same_channels = in_channels==out_channels
        self.is_res = is_res
        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.GELU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels),
            nn.GELU(),
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        if self.is_res:
            x1 = self.conv1(x)
            x2 = self.conv2(x1)
            # this adds on correct residual in case channels have increased
            if self.same_channels:
                out = x + x2
            else:
                out = x1 + x2 
            return out / 1.414
        else:
            x1 = self.conv1(x)
            x2 = self.conv2(x1)
            return x2

class UnetDown(nn.Module):
    def __init__(self, in_channels, out_channels):
        super(UnetDown, self).__init__()
        '''
        process and downscale the image feature maps
        '''
        layers = [ResidualConvBlock(in_channels, out_channels), nn.MaxPool2d(2)]
        self.model = nn.Sequential(*[ResidualConvBlock(in_channels, out_channels), nn.MaxPool2d(2)])

    def forward(self, x):
        return self.model(x)


class UnetUp(nn.Module):
    def __init__(self, in_channels, out_channels):
        super(UnetUp, self).__init__()
        '''
        process and upscale the image feature maps
        '''
        layers = [
            nn.ConvTranspose2d(in_channels, out_channels, 2, 2),
            ResidualConvBlock(out_channels, out_channels),
            ResidualConvBlock(out_channels, out_channels),
        ]
        self.model = nn.Sequential(*layers)

    def forward(self, x, skip):
        x = torch.cat((x, skip), 1)
        x = self.model(x)
        return x

class EmbedFC(nn.Module):
    def __init__(self, input_dim, emb_dim):
        super(EmbedFC, self).__init__()
        '''
        generic one layer FC NN for embedding things  
        '''
        self.input_dim = input_dim
        layers = [
            nn.Linear(input_dim, emb_dim),
            nn.GELU(),
            nn.Linear(emb_dim, emb_dim),
        ]
        self.model = nn.Sequential(*layers)

    def forward(self, x):
        x = x.view(-1, self.input_dim)
        return self.model(x)

class ContextUnet(nn.Module):
    def __init__(self, in_channels, n_feat = 256, n_classes=10):
        super(ContextUnet, self).__init__()

        self.in_channels = in_channels
        self.n_feat = n_feat
        self.n_classes = n_classes

        self.init_conv = ResidualConvBlock(in_channels, n_feat, is_res=True)

        self.down1 = UnetDown(n_feat, n_feat)
        self.down2 = UnetDown(n_feat, 2 * n_feat)

        self.to_vec = nn.Sequential(nn.AvgPool2d(7), nn.GELU())

        self.timeembed1 = EmbedFC(1, 2*n_feat)
        self.timeembed2 = EmbedFC(1, 1*n_feat)
        self.contextembed1 = EmbedFC(n_classes, 2*n_feat)
        self.contextembed2 = EmbedFC(n_classes, 1*n_feat)

        self.up0 = nn.Sequential(
            # nn.ConvTranspose2d(6 * n_feat, 2 * n_feat, 7, 7), # when concat temb and cemb end up w 6*n_feat
            nn.ConvTranspose2d(2 * n_feat, 2 * n_feat, 7, 7), # otherwise just have 2*n_feat
            nn.GroupNorm(8, 2 * n_feat),
            nn.ReLU(),
        )

        self.up1 = UnetUp(4 * n_feat, n_feat)
        self.up2 = UnetUp(2 * n_feat, n_feat)
        self.out = nn.Sequential(
            nn.Conv2d(2 * n_feat, n_feat, 3, 1, 1),
            nn.GroupNorm(8, n_feat),
            nn.ReLU(),
            nn.Conv2d(n_feat, self.in_channels, 3, 1, 1),
        )

    def forward(self, x, c, t, context_mask):
        # x is (noisy) image, c is context label, t is timestep, 
        # context_mask says which samples to block the context on

        x = self.init_conv(x)
        down1 = self.down1(x)
        down2 = self.down2(down1)
        hiddenvec = self.to_vec(down2)

        # convert context to one hot embedding
        c = nn.functional.one_hot(c, num_classes=self.n_classes).type(torch.float)
        
        # mask out context if context_mask == 1
        context_mask = context_mask[:, None]
        context_mask = context_mask.repeat(1,self.n_classes)
        context_mask = (-1*(1-context_mask)) # need to flip 0 <-> 1
        c = c * context_mask
        
        # embed context, time step
        cemb1 = self.contextembed1(c).view(-1, self.n_feat * 2, 1, 1)
        temb1 = self.timeembed1(t).view(-1, self.n_feat * 2, 1, 1)
        cemb2 = self.contextembed2(c).view(-1, self.n_feat, 1, 1)
        temb2 = self.timeembed2(t).view(-1, self.n_feat, 1, 1)

        # could concatenate the context embedding here instead of adaGN
        # hiddenvec = torch.cat((hiddenvec, temb1, cemb1), 1)

        up1 = self.up0(hiddenvec)
        # up2 = self.up1(up1, down2) # if want to avoid add and multiply embeddings
        up2 = self.up1(cemb1*up1+ temb1, down2)  # add and multiply embeddings
        up3 = self.up2(cemb2*up2+ temb2, down1)
        out = self.out(torch.cat((up3, x), 1))
        return out

"""## Schedule"""

def ddpm_schedules(beta1, beta2, T):
    """
    Returns pre-computed schedules for DDPM sampling, training process.
    """
    assert beta1 < beta2 < 1.0, "beta1 and beta2 must be in (0, 1)"

    beta_t = (beta2 - beta1) * torch.arange(0, T + 1, dtype=torch.float32) / T + beta1
    sqrt_beta_t = torch.sqrt(beta_t)
    alpha_t = 1 - beta_t
    log_alpha_t = torch.log(alpha_t)
    alphabar_t = torch.cumsum(log_alpha_t, dim=0).exp()

    sqrtab = torch.sqrt(alphabar_t)
    oneover_sqrta = 1 / torch.sqrt(alpha_t)

    sqrtmab = torch.sqrt(1 - alphabar_t)
    mab_over_sqrtmab_inv = (1 - alpha_t) / sqrtmab

    return {
        "alpha_t": alpha_t,  # \alpha_t
        "oneover_sqrta": oneover_sqrta,  # 1/\sqrt{\alpha_t}
        "sqrt_beta_t": sqrt_beta_t,  # \sqrt{\beta_t}
        "alphabar_t": alphabar_t,  # \bar{\alpha_t}
        "sqrtab": sqrtab,  # \sqrt{\bar{\alpha_t}}
        "sqrtmab": sqrtmab,  # \sqrt{1-\bar{\alpha_t}}
        "mab_over_sqrtmab": mab_over_sqrtmab_inv,  # (1-\alpha_t)/\sqrt{1-\bar{\alpha_t}}
    }

"""## DDPM"""

def show_tensor_image(image):
    reverse_transforms = transforms.Compose([
        transforms.Lambda(lambda t: (t + 1) / 2),
        transforms.Lambda(lambda t: t.permute(1, 2, 0)), # CHW to HWC
        transforms.Lambda(lambda t: t * 255.),
        transforms.Lambda(lambda t: t.numpy().astype(np.uint8)),
        transforms.ToPILImage(),
    ])

    # Take first image of batch
    if len(image.shape) == 4:
        image = image[0, :, :, :] 
    plt.imshow(reverse_transforms(image))

class DDPM(nn.Module):
    def __init__(self, nn_model, betas, n_T, device, drop_prob=0.1):
        super(DDPM, self).__init__()
        self.nn_model = nn_model.to(device)

        # register_buffer allows accessing dictionary produced by ddpm_schedules
        # e.g. can access self.sqrtab later
        for k, v in ddpm_schedules(betas[0], betas[1], n_T).items():
            self.register_buffer(k, v)

        self.n_T = n_T
        self.device = device
        self.drop_prob = drop_prob
        self.loss_mse = nn.MSELoss()

    def forward(self, x, c):
        """
        this method is used in training, so samples t and noise randomly
        """

        _ts = torch.randint(1, self.n_T, (x.shape[0],)).to(self.device)  # t ~ Uniform(0, n_T)
        noise = torch.randn_like(x)  # eps ~ N(0, 1)

        x_t = (
            self.sqrtab[_ts, None, None, None] * x
            + self.sqrtmab[_ts, None, None, None] * noise
        )  # This is the x_t, which is sqrt(alphabar) x_0 + sqrt(1-alphabar) * eps
        # We should predict the "error term" from this x_t. Loss is what we return.

        # dropout context with some probability
        context_mask = torch.bernoulli(torch.zeros_like(c)+self.drop_prob).to(self.device)
        
        # return MSE between added noise, and our predicted noise
        return self.loss_mse(noise, self.nn_model(x_t, c, _ts / self.n_T, context_mask))

    def sample(self, n_sample, size, device, guide_w = 0.0, show = False):
        # we follow the guidance sampling scheme described in 'Classifier-Free Diffusion Guidance'
        # to make the fwd passes efficient, we concat two versions of the dataset,
        # one with context_mask=0 and the other context_mask=1
        # we then mix the outputs with the guidance scale, w
        # where w>0 means more guidance

        x_i = torch.randn(n_sample, *size).to(device)  # x_T ~ N(0, 1), sample initial noise
        c_i = torch.arange(0,10).to(device) # context for us just cycles throught the mnist labels
        c_i = c_i.repeat(int(n_sample/c_i.shape[0]))

        # don't drop context at test time
        context_mask = torch.zeros_like(c_i).to(device)

        # double the batch
        c_i = c_i.repeat(2)
        context_mask = context_mask.repeat(2)
        context_mask[n_sample:] = 1. # makes second half of batch context free

        x_i_store = [] # keep track of generated steps in case want to plot something 
        # print()
        plot_pos = 1
        for i in range(self.n_T, 0, -1):
            # print(i)
            # if(show and (i-1)%(int(self.n_T/6)) == 0):
            #   # print(i)
            #   nothing = 1
              # plt.subplot(1, 6, i%(int(self.n_T/6))+1)
              # show_tensor_image(img.detach().cpu())
            # elif(i-1):
            #   continue
            print(f'sampling timestep {i}',end='\r')
            t_is = torch.tensor([i / self.n_T]).to(device)
            t_is = t_is.repeat(n_sample,1,1,1)

            # double batch
            x_i = x_i.repeat(2,1,1,1)
            t_is = t_is.repeat(2,1,1,1)

            z = torch.randn(n_sample, *size).to(device) if i > 1 else 0

            # split predictions and compute weighting
            # print(self.nn_model)
            eps = self.nn_model(x_i, c_i, t_is, context_mask)
            # print(eps)
            eps1 = eps[:n_sample]
            eps2 = eps[n_sample:]
            eps = (1+guide_w)*eps1 - guide_w*eps2
            x_i = x_i[:n_sample]
            x_i = (
                self.oneover_sqrta[i] * (x_i - eps * self.mab_over_sqrtmab[i])
                + self.sqrt_beta_t[i] * z
            )
            if (show and (i-1)%(int(self.n_T/6)) == 0):
              print(i)
              plt.subplot(1, 6, plot_pos)
              plot_pos += 1
              show_tensor_image(x_i[0].detach().cpu())
            # if i%20==0 or i==self.n_T or i<8:
            #     x_i_store.append(x_i.detach().cpu().numpy())
        plt.show()
        x_i_store = np.array(x_i_store)
        return x_i, x_i_store

"""## Define Models"""

n_T = 600 # 500
n_classes = 10
n_feat = 128 # 128 ok, 256 better (but slower)
lrate = 1e-4
if(torch.cuda.is_available()):
    device = torch.device("cuda")
else:
    device = torch.device('cpu')
    print('using CPU')
ddpm = DDPM(nn_model=ContextUnet(in_channels=3, n_feat=n_feat, n_classes=n_classes), betas=(1e-4, 0.02), n_T=n_T, device=device, drop_prob=0.1)
if(torch.cuda.is_available()):
    ddpm.to(device)
else:
    print('WARNING!!!!!!!!!!!!!! MPS CAN\'T BE USED')
optim = torch.optim.Adam(ddpm.parameters(), lr=lrate)

"""# Training

## Eval
"""

import os
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.transforms as transforms
from torch.utils.data import Dataset
from PIL import Image

MEAN =[0.5, 0.5, 0.5]     #[1, 1, 1]
STD = [0.5, 0.5, 0.5]     #[1, 1, 1]

def load_checkpoint(checkpoint_path, model):
	state = torch.load(checkpoint_path, map_location = "cuda")
	model.load_state_dict(state['state_dict'])
	print('model loaded from %s' % checkpoint_path)

class DATA(Dataset):
	def __init__(self, path):
		self.img_dir = path

		self.data = []
		self.labels = []
		for filename in os.listdir(self.img_dir):
			self.data.append(os.path.join(self.img_dir, filename))
			self.labels.append(int(filename[0]))

		self.transform = transforms.Compose([
							transforms.ToTensor(), # (H,W,C)->(C,H,W), [0,255]->[0, 1.0] RGB->RGB
							transforms.Normalize(MEAN, STD)
						])

	def __len__(self):
		return len(self.data)

	def __getitem__(self, idx):
		img_path = self.data[idx]
		img = Image.open(img_path).convert('RGB')

		return self.transform(img), self.labels[idx]

class Classifier(nn.Module):
	def __init__(self):
		super().__init__()
		self.conv1 = nn.Conv2d(3, 6, 5)
		self.pool = nn.MaxPool2d(2, 2)
		self.conv2 = nn.Conv2d(6, 16, 5)
		self.fc1 = nn.Linear(16 * 4 * 4, 128)
		self.fc2 = nn.Linear(128, 64)
		self.fc3 = nn.Linear(64, 10)

	def forward(self, x):
		x = self.pool(F.relu(self.conv1(x)))
		x = self.pool(F.relu(self.conv2(x)))
		x = torch.flatten(x, 1) # flatten all dimensions except batch
		x = F.relu(self.fc1(x))
		x = F.relu(self.fc2(x))
		x = self.fc3(x)
		return x

#print('===> prepare classifier ...')
net = Classifier()
path = "./Classifier.pth"
load_checkpoint(path, net)

# GPU enable
use_cuda = torch.cuda.is_available()
device = torch.device("cuda" if use_cuda else "cpu")
#print('Device used:', device)
net = net.to(device)

def result_checker(folder):
  data_loader = torch.utils.data.DataLoader(DATA(folder),
                          batch_size=32, 
                          num_workers=4,
                          shuffle=False)

  correct = 0
  total = 0
  net.eval()
  #print('===> start evaluation ...')
  with torch.no_grad():
    for idx, (imgs, labels) in enumerate(data_loader):
      imgs, labels = imgs.to(device), labels.to(device)
      output = net(imgs)
      _, pred = torch.max(output, 1)
      correct += (pred == labels).detach().sum().item()
      total += len(pred)
  print('acc = {} (correct/total = {}/{})'.format(float(correct)/total, correct, total))
  return float(correct)/total

"""## Train"""

# with torch.no_grad():
#   x_gen, x_gen_store = ddpm.sample(20, (3, 28, 28), device, guide_w=0)
#   print(x_gen.size())

n_epoch = 10
save_model = False
current_best = 0
save_dir = 'gen_img'
os.makedirs(save_dir, exist_ok = True)
# ws_test = [0.0, 0.5, 2.0] # strength of generative guidance
ck = 25
ddpm.load_state_dict(torch.load(f'/content/drive/MyDrive/hw2_models/ddpm_{ck}.ckpt'))
ws_test = [0.5]
reverse_transforms = transforms.Compose([
              # TF.Resize(28),
              transforms.Lambda(lambda t: (t + 1) / 2),
              transforms.Lambda(lambda t: t.permute(1, 2, 0)), # CHW to HWC
              transforms.Lambda(lambda t: t * 255.),
              transforms.Lambda(lambda t: t.numpy().astype(np.uint8)),
              transforms.ToPILImage(),
          ])
for ep in range(n_epoch):
    print(f'epoch {ep+ck+1}')
    

    # linear lrate decay
    optim.param_groups[0]['lr'] = lrate*(1-ep/n_epoch)
    pbar = tqdm(dataloader)
    loss_ema = None
    for x, c in pbar:
        optim.zero_grad()
        x = x.to(device)
        c = c.to(device)
        loss = ddpm(x, c)
        loss.backward()
        if loss_ema is None:
            loss_ema = loss.item()
        else:
            loss_ema = 0.95 * loss_ema + 0.05 * loss.item()
        pbar.set_description(f"loss: {loss_ema:.4f}")
        optim.step()
    
    # for eval, save an image of currently generated samples (top rows)
    # followed by real images (bottom rows)
    ddpm.eval()
    with torch.no_grad():
        n_sample = 100*n_classes
        for w_i, w in enumerate(ws_test):
            # if ep%5 != 0:
            #     sample_num = 20
            # else:
            #     sample_num = 50
            sample_num = 50
            progress = tqdm(total = sample_num)
            for sample_idx in range(sample_num):
                x_gen, x_gen_store = ddpm.sample(20, (3, 28, 28), device, guide_w=w, show = not bool(sample_idx))
                # if not sample_idx:
                #   print(torch.max(x_gen), torch.min(x_gen))
                progress.update(1)
                for i in range(x_gen.size()[0]):
                    save_image(x_gen[i].detach().cpu(), f'{save_dir}/{i%10}_{(sample_idx*2+1+i//10):03d}.png')
                    # im = reverse_transforms(x_gen[i].detach().cpu())
                    # im.save(f'{save_dir}/{i%10}_{(sample_idx*2+1+i//10):03d}.png')
    ddpm.train()
            # print(x_gen.size())
            # append some real images at bottom, order by class also
            # x_real = torch.Tensor(x_gen.shape).to(device)
            # for k in range(n_classes):
            #     for j in range(int(n_sample/n_classes)):
            #         try: 
            #             idx = torch.squeeze((c == k).nonzero())[j]
            #         except:
            #             idx = 0
            #         x_real[k+(j*n_classes)] = x[idx]

            # x_all = torch.cat([x_gen, x_real])
            # grid = make_grid(x_all*-1 + 1, nrow=10)
            # save_image(grid, save_dir + f"image_ep{ep}_w{w}.png")
            # print('saved image at ' + save_dir + f"image_ep{ep}_w{w}.png")
    acc = result_checker('/content/gen_img')
    if acc > current_best:
        current_best = acc
        print('models save')
        torch.save(ddpm.state_dict(), f'/content/drive/MyDrive/hw2_models/ddpm_{ep+ck+1}.ckpt')